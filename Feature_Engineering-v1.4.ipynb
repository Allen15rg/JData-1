{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "版本一\n",
    "\n",
    "* get_action_feat(): 最后保留类别属性，此外在对不同天拼接时注意on=['user_id', 'sku_id', 'cate']\n",
    "* get_accumulate_user_feat()：平滑转化率计算、标准差计算前的空值填充，平滑recent(怎么做更合理)，days_interval->day\n",
    "* user_cate：计算cate各项行为占比涉及除法会出现空值，这里对于除法可以做一个对数操作\n",
    "* product_basic：保留该特征，若使用的话空值填充需要注意，合并后填充空值为-1，对a1,a2,a3表示未知，brand需要看看是否存在这个品牌代号（没有），但是这里有一个问题，原始的a1,a2,a3均经过独热编码处理，所以这里并不是单纯的填充为-1，而是还是填充为0(刚好brand也没有这个值),此外注意拼接时on=['sku_id', 'cate']\n",
    "* get_accumulate_product_feat：修改转化率计算，在make_action里注意修改日期跨度问题,计数保留所有行为，标准差计算前的空值填充\n",
    "* get_accumulate_cate_feat()：修改转化率计算，修改日期跨度问题,计数保留全部，标准差计算前的空值填充\n",
    "* get_comment_product_date()：评论独热少了一个评论数为0的情况（但是对于部分时间窗可能并不存在comment为0的状态，所以需要处理），拼接后填充空值，取评论时的日期操作是多余的，直接取等即可，start_date多余，此外需分析comment数据集中包含的商品是否全部来自product（不是）\n",
    "* 最后打标签这块，先还是对用户购买类别8的商品对打标签，需要实验和思考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备\n",
    "* 导入相应的库\n",
    "* 设置文件路径等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:17.345239Z",
     "start_time": "2017-05-16T21:16:15.334700Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:17.352419Z",
     "start_time": "2017-05-16T21:16:17.346954Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# action_1_path = r'data/JData_Action_201602.csv'\n",
    "# action_2_path = r'data/JData_Action_201603.csv'\n",
    "# action_3_path = r'data/JData_Action_201604.csv'\n",
    "action_1_path = r'data/actions1.csv'\n",
    "action_2_path = r'data/actions2.csv'\n",
    "action_3_path = r'data/actions3.csv'\n",
    "comment_path = r'data/JData_Comment.csv'\n",
    "product_path = r'data/JData_Product.csv'\n",
    "# user_path = r'data/JData_User.csv'\n",
    "user_path = r'data/user.csv'\n",
    "\n",
    "comment_date = [\n",
    "    \"2016-02-01\", \"2016-02-08\", \"2016-02-15\", \"2016-02-22\", \"2016-02-29\",\n",
    "    \"2016-03-07\", \"2016-03-14\", \"2016-03-21\", \"2016-03-28\", \"2016-04-04\",\n",
    "    \"2016-04-11\", \"2016-04-15\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:17.463427Z",
     "start_time": "2017-05-16T21:16:17.353703Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions_1():\n",
    "    action = pd.read_csv(action_1_path)\n",
    "    return action\n",
    "\n",
    "def get_actions_2():\n",
    "    action2 = pd.read_csv(action_2_path)\n",
    "    return action2\n",
    "\n",
    "def get_actions_3():\n",
    "    action3 = pd.read_csv(action_3_path)\n",
    "    return action3\n",
    "# 读取并拼接所有行为记录文件\n",
    "def get_all_action():\n",
    "    action_1 = get_actions_1()\n",
    "    action_2 = get_actions_2()\n",
    "    action_3 = get_actions_3()\n",
    "    actions = pd.concat([action_1, action_2, action_3]) # type: pd.DataFrame\n",
    "#     actions = pd.read_csv(action_path)\n",
    "    return actions\n",
    "\n",
    "# 获取某个时间段的行为记录\n",
    "def get_actions(start_date, end_date, all_actions):\n",
    "    \"\"\"\n",
    "    :param start_date:\n",
    "    :param end_date:\n",
    "    :return: actions: pd.Dataframe\n",
    "    \"\"\"\n",
    "    actions = all_actions[(all_actions.time >= start_date) & (all_actions.time < end_date)].copy()\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户特征\n",
    "### 用户基本特征\n",
    "获取基本的用户特征，基于用户本身属性多为类别特征的特点，对age,sex,usr_lv_cd进行独热编码操作，对于用户注册时间暂时不处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:18.813768Z",
     "start_time": "2017-05-16T21:16:17.467022Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def get_basic_user_feat():\n",
    "    # 针对年龄的中文字符问题处理，首先是读入的时候编码，填充空值，然后将其数值化，最后独热编码，此外对于sex也进行了数值类型转换\n",
    "    user = pd.read_csv(user_path, encoding='gbk')\n",
    "#     user['age'].fillna('-1', inplace=True)\n",
    "#     user['sex'].fillna(2, inplace=True)\n",
    "    user['sex'] = user['sex'].astype(int)    \n",
    "    user['age'] = user['age'].astype(unicode)\n",
    "    le = preprocessing.LabelEncoder()    \n",
    "    age_df = le.fit_transform(user['age'])\n",
    "#     print list(le.classes_)\n",
    "\n",
    "    age_df = pd.get_dummies(age_df, prefix='age')\n",
    "    sex_df = pd.get_dummies(user['sex'], prefix='sex')\n",
    "    user_lv_df = pd.get_dummies(user['user_lv_cd'], prefix='user_lv_cd')\n",
    "    user = pd.concat([user['user_id'], age_df, sex_df, user_lv_df], axis=1)\n",
    "    return user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 商品特征\n",
    "### 商品基本特征\n",
    "根据商品文件获取基本的特征，针对属性a1,a2,a3进行独热编码，商品类别和品牌直接作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:18.826294Z",
     "start_time": "2017-05-16T21:16:18.818260Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_basic_product_feat():\n",
    "    product = pd.read_csv(product_path)\n",
    "    attr1_df = pd.get_dummies(product[\"a1\"], prefix=\"a1\")\n",
    "    attr2_df = pd.get_dummies(product[\"a2\"], prefix=\"a2\")\n",
    "    attr3_df = pd.get_dummies(product[\"a3\"], prefix=\"a3\")\n",
    "    product = pd.concat([product[['sku_id', 'cate', 'brand']], attr1_df, attr2_df, attr3_df], axis=1)\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评论特征\n",
    "\n",
    "* 分时间段\n",
    "* 对评论数进行独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:18.880657Z",
     "start_time": "2017-05-16T21:16:18.827667Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments_product_feat(end_date):\n",
    "    comments = pd.read_csv(comment_path)\n",
    "    comment_date_end = end_date\n",
    "    comment_date_begin = comment_date[0]\n",
    "    for date in reversed(comment_date):\n",
    "        if date < comment_date_end:\n",
    "            comment_date_begin = date\n",
    "            break\n",
    "    comments = comments[comments.dt==comment_date_begin]\n",
    "    df = pd.get_dummies(comments['comment_num'], prefix='comment_num')\n",
    "    # 为了防止某个时间段不具备评论数为0的情况（测试集出现过这种情况）\n",
    "    for i in range(0, 5):\n",
    "        if 'comment_num_' + str(i) not in df.columns:\n",
    "            df['comment_num_' + str(i)] = 0\n",
    "    df = df[['comment_num_0', 'comment_num_1', 'comment_num_2', 'comment_num_3', 'comment_num_4']]\n",
    "    \n",
    "    comments = pd.concat([comments, df], axis=1) # type: pd.DataFrame\n",
    "        #del comments['dt']\n",
    "        #del comments['comment_num']\n",
    "    comments = comments[['sku_id', 'has_bad_comment', 'bad_comment_rate','comment_num_0', 'comment_num_1', \n",
    "                         'comment_num_2', 'comment_num_3', 'comment_num_4']]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行为特征\n",
    "\n",
    "* 分时间段\n",
    "* 对行为类别进行独热编码\n",
    "* 分别按照用户-类别行为分组和用户-类别-商品行为分组统计，然后计算\n",
    " * 用户对同类别下其他商品的行为计数\n",
    " * 针对用户对同类别下目标商品的行为计数与该时间段的行为均值作差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:18.950287Z",
     "start_time": "2017-05-16T21:16:18.882812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_feat(start_date, end_date, all_actions, i):\n",
    "    actions = get_actions(start_date, end_date, all_actions)\n",
    "    actions = actions[['user_id', 'sku_id', 'cate','type']]\n",
    "    # 不同时间累积的行为计数（3,5,7,10,15,21,30）\n",
    "    df = pd.get_dummies(actions['type'], prefix='action_before_%s' %i)\n",
    "    before_date = 'action_before_%s' %i\n",
    "    actions = pd.concat([actions, df], axis=1)  # type: pd.DataFrame\n",
    "    # 分组统计，用户-类别-商品,不同用户对不同类别下商品的行为计数\n",
    "    actions = actions.groupby(['user_id', 'sku_id','cate'], as_index=False).sum()\n",
    "    # 分组统计，用户-类别，不同用户对不同商品类别的行为计数\n",
    "    user_cate = actions.groupby(['user_id','cate'], as_index=False).sum()\n",
    "    del user_cate['sku_id']\n",
    "    del user_cate['type']\n",
    "    actions = pd.merge(actions, user_cate, how='left', on=['user_id','cate'])\n",
    "    #本类别下其他商品点击量\n",
    "    # 前述两种分组含有相同名称的不同行为的计数，系统会自动针对名称调整添加后缀,x,y，所以这里作差统计的是同一类别下其他商品的行为计数\n",
    "    actions[before_date+'_1_y'] = actions[before_date+'_1_y'] - actions[before_date+'_1_x']\n",
    "    actions[before_date+'_2_y'] = actions[before_date+'_2_y'] - actions[before_date+'_2_x']\n",
    "    actions[before_date+'_3_y'] = actions[before_date+'_3_y'] - actions[before_date+'_3_x']\n",
    "    actions[before_date+'_4_y'] = actions[before_date+'_4_y'] - actions[before_date+'_4_x']\n",
    "    actions[before_date+'_5_y'] = actions[before_date+'_5_y'] - actions[before_date+'_5_x']\n",
    "    actions[before_date+'_6_y'] = actions[before_date+'_6_y'] - actions[before_date+'_6_x']\n",
    "    # 统计用户对不同类别下商品计数与该类别下商品行为计数均值（对时间）的差值\n",
    "    actions[before_date+'minus_mean_1'] = actions[before_date+'_1_x'] - (actions[before_date+'_1_x']/i)\n",
    "    actions[before_date+'minus_mean_2'] = actions[before_date+'_2_x'] - (actions[before_date+'_2_x']/i)\n",
    "    actions[before_date+'minus_mean_3'] = actions[before_date+'_3_x'] - (actions[before_date+'_3_x']/i)\n",
    "    actions[before_date+'minus_mean_4'] = actions[before_date+'_4_x'] - (actions[before_date+'_4_x']/i)\n",
    "    actions[before_date+'minus_mean_5'] = actions[before_date+'_5_x'] - (actions[before_date+'_5_x']/i)\n",
    "    actions[before_date+'minus_mean_6'] = actions[before_date+'_6_x'] - (actions[before_date+'_6_x']/i)\n",
    "    del actions['type']\n",
    "    # 保留cate特征\n",
    "#     del actions['cate']\n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户-行为\n",
    "#### 累积用户特征\n",
    "\n",
    "* 分时间段\n",
    "* 用户不同行为的\n",
    " * 购买转化率\n",
    " * 均值\n",
    " * 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.036710Z",
     "start_time": "2017-05-16T21:16:18.952454Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accumulate_user_feat(end_date, all_actions, day):\n",
    "    start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=day)\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    before_date = 'user_action_%s' % day\n",
    "\n",
    "    feature = [\n",
    "        'user_id', before_date + '_1', before_date + '_2', before_date + '_3',\n",
    "        before_date + '_4', before_date + '_5', before_date + '_6',\n",
    "        before_date + '_1_ratio', before_date + '_2_ratio',\n",
    "        before_date + '_3_ratio', before_date + '_5_ratio',\n",
    "        before_date + '_6_ratio', before_date + '_1_mean',\n",
    "        before_date + '_2_mean', before_date + '_3_mean',\n",
    "        before_date + '_4_mean', before_date + '_5_mean',\n",
    "        before_date + '_6_mean', before_date + '_1_std',\n",
    "        before_date + '_2_std', before_date + '_3_std', before_date + '_4_std',\n",
    "        before_date + '_5_std', before_date + '_6_std'\n",
    "    ]\n",
    "\n",
    "    actions = get_actions(start_date, end_date, all_actions)\n",
    "    df = pd.get_dummies(actions['type'], prefix=before_date)\n",
    "\n",
    "#     actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date())\n",
    "\n",
    "    actions = pd.concat([actions[['user_id', 'date']], df], axis=1)\n",
    "    # 分组统计，用户不同日期的行为计算标准差\n",
    "    actions_date = actions.groupby(['user_id', 'date']).sum()\n",
    "    actions_date = actions_date.unstack()\n",
    "    actions_date.fillna(0, inplace=True)\n",
    "    action_1 = np.std(actions_date[before_date + '_1'], axis=1)\n",
    "    action_1 = action_1.to_frame()\n",
    "    action_1.columns = [before_date + '_1_std']\n",
    "    action_2 = np.std(actions_date[before_date + '_2'], axis=1)\n",
    "    action_2 = action_2.to_frame()\n",
    "    action_2.columns = [before_date + '_2_std']\n",
    "    action_3 = np.std(actions_date[before_date + '_3'], axis=1)\n",
    "    action_3 = action_3.to_frame()\n",
    "    action_3.columns = [before_date + '_3_std']\n",
    "    action_4 = np.std(actions_date[before_date + '_4'], axis=1)\n",
    "    action_4 = action_4.to_frame()\n",
    "    action_4.columns = [before_date + '_4_std']\n",
    "    action_5 = np.std(actions_date[before_date + '_5'], axis=1)\n",
    "    action_5 = action_5.to_frame()\n",
    "    action_5.columns = [before_date + '_5_std']\n",
    "    action_6 = np.std(actions_date[before_date + '_6'], axis=1)\n",
    "    action_6 = action_6.to_frame()\n",
    "    action_6.columns = [before_date + '_6_std']\n",
    "    actions_date = pd.concat(\n",
    "        [action_1, action_2, action_3, action_4, action_5, action_6], axis=1)\n",
    "    actions_date['user_id'] = actions_date.index\n",
    "    # 分组统计，按用户分组，统计用户各项行为的转化率、均值\n",
    "    actions = actions.groupby(['user_id'], as_index=False).sum()\n",
    "#     days_interal = (datetime.strptime(end_date, '%Y-%m-%d') -\n",
    "#                     datetime.strptime(start_date, '%Y-%m-%d')).days\n",
    "    # 转化率\n",
    "#     actions[before_date + '_1_ratio'] = actions[before_date +\n",
    "#                                                 '_4'] / actions[before_date +\n",
    "#                                                                 '_1']\n",
    "#     actions[before_date + '_2_ratio'] = actions[before_date +\n",
    "#                                                 '_4'] / actions[before_date +\n",
    "#                                                                 '_2']\n",
    "#     actions[before_date + '_3_ratio'] = actions[before_date +\n",
    "#                                                 '_4'] / actions[before_date +\n",
    "#                                                                 '_3']\n",
    "#     actions[before_date + '_5_ratio'] = actions[before_date +\n",
    "#                                                 '_4'] / actions[before_date +\n",
    "#                                                                 '_5']\n",
    "#     actions[before_date + '_6_ratio'] = actions[before_date +\n",
    "#                                                 '_4'] / actions[before_date +\n",
    "#                                                                 '_6']\n",
    "    actions[before_date + '_1_ratio'] =  np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_1'])\n",
    "    actions[before_date + '_2_ratio'] =  np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_2'])\n",
    "    actions[before_date + '_3_ratio'] =  np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_3'])\n",
    "    actions[before_date + '_5_ratio'] =  np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_5'])\n",
    "    actions[before_date + '_6_ratio'] =  np.log(1 + actions[before_date + '_4']) - np.log(1 + actions[before_date +'_6'])\n",
    "    # 均值\n",
    "    actions[before_date + '_1_mean'] = actions[before_date + '_1'] / day\n",
    "    actions[before_date + '_2_mean'] = actions[before_date + '_2'] / day\n",
    "    actions[before_date + '_3_mean'] = actions[before_date + '_3'] / day\n",
    "    actions[before_date + '_4_mean'] = actions[before_date + '_4'] / day\n",
    "    actions[before_date + '_5_mean'] = actions[before_date + '_5'] / day\n",
    "    actions[before_date + '_6_mean'] = actions[before_date + '_6'] / day\n",
    "    actions = pd.merge(actions, actions_date, how='left', on='user_id')\n",
    "    actions = actions[feature]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户近期行为特征\n",
    "\n",
    "在上面针对用户进行累积特征提取的基础上，分别提取用户近一个月、近三天的特征，然后提取一个月内用户除去最近三天的行为占据一个月的行为的比重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.108485Z",
     "start_time": "2017-05-16T21:16:19.037711Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recent_user_feat(end_date, all_actions):\n",
    "    actions_3 = get_accumulate_user_feat(end_date, all_actions, 3)\n",
    "    actions_30 = get_accumulate_user_feat(end_date, all_actions, 30)\n",
    "    actions = pd.merge(actions_3, actions_30, how ='left', on='user_id')\n",
    "    del actions_3\n",
    "    del actions_30\n",
    "    \n",
    "    actions['recent_action1'] =  np.log(1 + actions['user_action_30_1']-actions['user_action_3_1']) - np.log(1 + actions['user_action_30_1'])\n",
    "    actions['recent_action2'] =  np.log(1 + actions['user_action_30_2']-actions['user_action_3_2']) - np.log(1 + actions['user_action_30_2'])\n",
    "    actions['recent_action3'] =  np.log(1 + actions['user_action_30_3']-actions['user_action_3_3']) - np.log(1 + actions['user_action_30_3'])\n",
    "    actions['recent_action4'] =  np.log(1 + actions['user_action_30_4']-actions['user_action_3_4']) - np.log(1 + actions['user_action_30_4'])\n",
    "    actions['recent_action5'] =  np.log(1 + actions['user_action_30_5']-actions['user_action_3_5']) - np.log(1 + actions['user_action_30_5'])\n",
    "    actions['recent_action6'] =  np.log(1 + actions['user_action_30_6']-actions['user_action_3_6']) - np.log(1 + actions['user_action_30_6'])\n",
    "    \n",
    "#     actions['recent_action1'] = (actions['user_action_30_1']-actions['user_action_3_1'])/actions['user_action_30_1']\n",
    "#     actions['recent_action2'] = (actions['user_action_30_2']-actions['user_action_3_2'])/actions['user_action_30_2']\n",
    "#     actions['recent_action3'] = (actions['user_action_30_3']-actions['user_action_3_3'])/actions['user_action_30_3']\n",
    "#     actions['recent_action4'] = (actions['user_action_30_4']-actions['user_action_3_4'])/actions['user_action_30_4']\n",
    "#     actions['recent_action5'] = (actions['user_action_30_5']-actions['user_action_3_5'])/actions['user_action_30_5']\n",
    "#     actions['recent_action6'] = (actions['user_action_30_6']-actions['user_action_3_6'])/actions['user_action_30_6']\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户对同类别下各种商品的行为\n",
    "* 用户对各个类别的各项行为操作统计\n",
    "* 用户对各个类别操作行为统计占对所有类别操作行为统计的比重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.184116Z",
     "start_time": "2017-05-16T21:16:19.110701Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#增加了用户对不同类别的交互特征\n",
    "def get_user_cate_feature(start_date, end_date, all_actions):\n",
    "    actions = get_actions(start_date, end_date, all_actions)\n",
    "    actions = actions[['user_id', 'cate', 'type']]\n",
    "    df = pd.get_dummies(actions['type'], prefix='type')\n",
    "    actions = pd.concat([actions[['user_id', 'cate']], df], axis=1)\n",
    "    actions = actions.groupby(['user_id', 'cate']).sum()\n",
    "    actions = actions.unstack()\n",
    "    actions.columns = actions.columns.swaplevel(0, 1)\n",
    "    actions.columns = actions.columns.droplevel()\n",
    "    actions.columns = [\n",
    "        'cate_4_type1', 'cate_5_type1', 'cate_6_type1', 'cate_7_type1',\n",
    "        'cate_8_type1', 'cate_9_type1', 'cate_10_type1', 'cate_11_type1',\n",
    "        'cate_4_type2', 'cate_5_type2', 'cate_6_type2', 'cate_7_type2',\n",
    "        'cate_8_type2', 'cate_9_type2', 'cate_10_type2', 'cate_11_type2',\n",
    "        'cate_4_type3', 'cate_5_type3', 'cate_6_type3', 'cate_7_type3',\n",
    "        'cate_8_type3', 'cate_9_type3', 'cate_10_type3', 'cate_11_type3',\n",
    "        'cate_4_type4', 'cate_5_type4', 'cate_6_type4', 'cate_7_type4',\n",
    "        'cate_8_type4', 'cate_9_type4', 'cate_10_type4', 'cate_11_type4',\n",
    "        'cate_4_type5', 'cate_5_type5', 'cate_6_type5', 'cate_7_type5',\n",
    "        'cate_8_type5', 'cate_9_type5', 'cate_10_type5', 'cate_11_type5',\n",
    "        'cate_4_type6', 'cate_5_type6', 'cate_6_type6', 'cate_7_type6',\n",
    "        'cate_8_type6', 'cate_9_type6', 'cate_10_type6', 'cate_11_type6'\n",
    "    ]\n",
    "    actions = actions.fillna(0)\n",
    "    actions['cate_action_sum'] = actions.sum(axis=1)\n",
    "    actions['cate8_percentage'] = (\n",
    "        actions['cate_8_type1'] + actions['cate_8_type2'] +\n",
    "        actions['cate_8_type3'] + actions['cate_8_type4'] +\n",
    "        actions['cate_8_type5'] + actions['cate_8_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate4_percentage'] = (\n",
    "        actions['cate_4_type1'] + actions['cate_4_type2'] +\n",
    "        actions['cate_4_type3'] + actions['cate_4_type4'] +\n",
    "        actions['cate_4_type5'] + actions['cate_4_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate5_percentage'] = (\n",
    "        actions['cate_5_type1'] + actions['cate_5_type2'] +\n",
    "        actions['cate_5_type3'] + actions['cate_5_type4'] +\n",
    "        actions['cate_5_type5'] + actions['cate_5_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate6_percentage'] = (\n",
    "        actions['cate_6_type1'] + actions['cate_6_type2'] +\n",
    "        actions['cate_6_type3'] + actions['cate_6_type4'] +\n",
    "        actions['cate_6_type5'] + actions['cate_6_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate7_percentage'] = (\n",
    "        actions['cate_7_type1'] + actions['cate_7_type2'] +\n",
    "        actions['cate_7_type3'] + actions['cate_7_type4'] +\n",
    "        actions['cate_7_type5'] + actions['cate_7_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate9_percentage'] = (\n",
    "        actions['cate_9_type1'] + actions['cate_9_type2'] +\n",
    "        actions['cate_9_type3'] + actions['cate_9_type4'] +\n",
    "        actions['cate_9_type5'] + actions['cate_9_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate10_percentage'] = (\n",
    "        actions['cate_10_type1'] + actions['cate_10_type2'] +\n",
    "        actions['cate_10_type3'] + actions['cate_10_type4'] +\n",
    "        actions['cate_10_type5'] + actions['cate_10_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "    actions['cate11_percentage'] = (\n",
    "        actions['cate_11_type1'] + actions['cate_11_type2'] +\n",
    "        actions['cate_11_type3'] + actions['cate_11_type4'] +\n",
    "        actions['cate_11_type5'] + actions['cate_11_type6']\n",
    "    ) / actions['cate_action_sum']\n",
    "\n",
    "    actions['cate8_type1_percentage'] = np.log(\n",
    "        1 + actions['cate_8_type1']) - np.log(\n",
    "            1 + actions['cate_8_type1'] + actions['cate_4_type1'] +\n",
    "            actions['cate_5_type1'] + actions['cate_6_type1'] +\n",
    "            actions['cate_7_type1'] + actions['cate_9_type1'] +\n",
    "            actions['cate_10_type1'] + actions['cate_11_type1'])\n",
    "\n",
    "    actions['cate8_type2_percentage'] = np.log(\n",
    "        1 + actions['cate_8_type2']) - np.log(\n",
    "            1 + actions['cate_8_type2'] + actions['cate_4_type2'] +\n",
    "            actions['cate_5_type2'] + actions['cate_6_type2'] +\n",
    "            actions['cate_7_type2'] + actions['cate_9_type2'] +\n",
    "            actions['cate_10_type2'] + actions['cate_11_type2'])\n",
    "    actions['cate8_type3_percentage'] = np.log(\n",
    "        1 + actions['cate_8_type3']) - np.log(\n",
    "            1 + actions['cate_8_type3'] + actions['cate_4_type3'] +\n",
    "            actions['cate_5_type3'] + actions['cate_6_type3'] +\n",
    "            actions['cate_7_type3'] + actions['cate_9_type3'] +\n",
    "            actions['cate_10_type3'] + actions['cate_11_type3'])\n",
    "    actions['cate8_type4_percentage'] = np.log(\n",
    "        1 + actions['cate_8_type4']) - np.log(\n",
    "            1 + actions['cate_8_type4'] + actions['cate_4_type4'] +\n",
    "            actions['cate_5_type4'] + actions['cate_6_type4'] +\n",
    "            actions['cate_7_type4'] + actions['cate_9_type4'] +\n",
    "            actions['cate_10_type4'] + actions['cate_11_type4'])\n",
    "    actions['cate8_type5_percentage'] = np.log(\n",
    "        1 + actions['cate_8_type5']) - np.log(\n",
    "            1 + actions['cate_8_type5'] + actions['cate_4_type5'] +\n",
    "            actions['cate_5_type5'] + actions['cate_6_type5'] +\n",
    "            actions['cate_7_type5'] + actions['cate_9_type5'] +\n",
    "            actions['cate_10_type5'] + actions['cate_11_type5'])\n",
    "    actions['cate8_type6_percentage'] = np.log(\n",
    "        1 + actions['cate_8_type6']) - np.log(\n",
    "            1 + actions['cate_8_type6'] + actions['cate_4_type6'] +\n",
    "            actions['cate_5_type6'] + actions['cate_6_type6'] +\n",
    "            actions['cate_7_type6'] + actions['cate_9_type6'] +\n",
    "            actions['cate_10_type6'] + actions['cate_11_type6'])\n",
    "    actions['user_id'] = actions.index\n",
    "    actions = actions[[\n",
    "        'user_id', 'cate8_percentage', 'cate4_percentage', 'cate5_percentage',\n",
    "        'cate6_percentage', 'cate7_percentage', 'cate9_percentage',\n",
    "        'cate10_percentage', 'cate11_percentage', 'cate8_type1_percentage',\n",
    "        'cate8_type2_percentage', 'cate8_type3_percentage',\n",
    "        'cate8_type4_percentage', 'cate8_type5_percentage',\n",
    "        'cate8_type6_percentage'\n",
    "    ]]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 商品-行为\n",
    "#### 累积商品特征\n",
    "* 分时间段\n",
    "* 针对商品的不同行为的\n",
    " * 购买转化率\n",
    " * 均值\n",
    " * 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.251417Z",
     "start_time": "2017-05-16T21:16:19.185064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accumulate_product_feat(start_date, end_date, all_actions):\n",
    "    feature = [\n",
    "        'sku_id', 'product_action_1', 'product_action_2',\n",
    "        'product_action_3', 'product_action_4',\n",
    "        'product_action_5', 'product_action_6',\n",
    "        'product_action_1_ratio', 'product_action_2_ratio',\n",
    "        'product_action_3_ratio', 'product_action_5_ratio',\n",
    "        'product_action_6_ratio', 'product_action_1_mean',\n",
    "        'product_action_2_mean', 'product_action_3_mean',\n",
    "        'product_action_4_mean', 'product_action_5_mean',\n",
    "        'product_action_6_mean', 'product_action_1_std',\n",
    "        'product_action_2_std', 'product_action_3_std', 'product_action_4_std',\n",
    "        'product_action_5_std', 'product_action_6_std'\n",
    "    ]\n",
    "\n",
    "    actions = get_actions(start_date, end_date, all_actions)\n",
    "    df = pd.get_dummies(actions['type'], prefix='product_action')\n",
    "    # 按照商品-日期分组，计算某个时间段该商品的各项行为的标准差\n",
    "#     actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date())\n",
    "    actions = pd.concat([actions[['sku_id', 'date']], df], axis=1)\n",
    "    actions_date = actions.groupby(['sku_id', 'date']).sum()\n",
    "    actions_date = actions_date.unstack()\n",
    "    actions_date.fillna(0, inplace=True)\n",
    "    action_1 = np.std(actions_date['product_action_1'], axis=1)\n",
    "    action_1 = action_1.to_frame()\n",
    "    action_1.columns = ['product_action_1_std']\n",
    "    action_2 = np.std(actions_date['product_action_2'], axis=1)\n",
    "    action_2 = action_2.to_frame()\n",
    "    action_2.columns = ['product_action_2_std']\n",
    "    action_3 = np.std(actions_date['product_action_3'], axis=1)\n",
    "    action_3 = action_3.to_frame()\n",
    "    action_3.columns = ['product_action_3_std']\n",
    "    action_4 = np.std(actions_date['product_action_4'], axis=1)\n",
    "    action_4 = action_4.to_frame()\n",
    "    action_4.columns = ['product_action_4_std']\n",
    "    action_5 = np.std(actions_date['product_action_5'], axis=1)\n",
    "    action_5 = action_5.to_frame()\n",
    "    action_5.columns = ['product_action_5_std']\n",
    "    action_6 = np.std(actions_date['product_action_6'], axis=1)\n",
    "    action_6 = action_6.to_frame()\n",
    "    action_6.columns = ['product_action_6_std']\n",
    "    actions_date = pd.concat(\n",
    "        [action_1, action_2, action_3, action_4, action_5, action_6], axis=1)\n",
    "    actions_date['sku_id'] = actions_date.index\n",
    "\n",
    "    actions = actions.groupby(['sku_id'], as_index=False).sum()\n",
    "    days_interal = (datetime.strptime(end_date, '%Y-%m-%d') - datetime.strptime(start_date, '%Y-%m-%d')).days\n",
    "    # 针对商品分组，计算购买转化率\n",
    "#     actions['product_action_1_ratio'] = actions['product_action_4'] / actions[\n",
    "#         'product_action_1']\n",
    "#     actions['product_action_2_ratio'] = actions['product_action_4'] / actions[\n",
    "#         'product_action_2']\n",
    "#     actions['product_action_3_ratio'] = actions['product_action_4'] / actions[\n",
    "#         'product_action_3']\n",
    "#     actions['product_action_5_ratio'] = actions['product_action_4'] / actions[\n",
    "#         'product_action_5']\n",
    "#     actions['product_action_6_ratio'] = actions['product_action_4'] / actions[\n",
    "#         'product_action_6']\n",
    "    actions['product_action_1_ratio'] =  np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_1'])\n",
    "    actions['product_action_2_ratio'] =  np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_2'])\n",
    "    actions['product_action_3_ratio'] =  np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_3'])\n",
    "    actions['product_action_5_ratio'] =  np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_5'])\n",
    "    actions['product_action_6_ratio'] =  np.log(1 + actions['product_action_4']) - np.log(1 + actions['product_action_6'])\n",
    "    # 计算各种行为的均值\n",
    "    actions['product_action_1_mean'] = actions[\n",
    "        'product_action_1'] / days_interal\n",
    "    actions['product_action_2_mean'] = actions[\n",
    "        'product_action_2'] / days_interal\n",
    "    actions['product_action_3_mean'] = actions[\n",
    "        'product_action_3'] / days_interal\n",
    "    actions['product_action_4_mean'] = actions[\n",
    "        'product_action_4'] / days_interal\n",
    "    actions['product_action_5_mean'] = actions[\n",
    "        'product_action_5'] / days_interal\n",
    "    actions['product_action_6_mean'] = actions[\n",
    "        'product_action_6'] / days_interal\n",
    "    actions = pd.merge(actions, actions_date, how='left', on='sku_id')\n",
    "    actions = actions[feature]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类别特征\n",
    "分时间段下各个商品类别的\n",
    "* 购买转化率\n",
    "* 标准差\n",
    "* 均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.317213Z",
     "start_time": "2017-05-16T21:16:19.252514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accumulate_cate_feat(start_date, end_date, all_actions):\n",
    "    feature = ['cate','cate_action_1', 'cate_action_2', 'cate_action_3', 'cate_action_4', 'cate_action_5', \n",
    "               'cate_action_6', 'cate_action_1_ratio', 'cate_action_2_ratio', \n",
    "               'cate_action_3_ratio', 'cate_action_5_ratio', 'cate_action_6_ratio', 'cate_action_1_mean',\n",
    "               'cate_action_2_mean', 'cate_action_3_mean', 'cate_action_4_mean', 'cate_action_5_mean',\n",
    "               'cate_action_6_mean', 'cate_action_1_std', 'cate_action_2_std', 'cate_action_3_std',\n",
    "               'cate_action_4_std', 'cate_action_5_std', 'cate_action_6_std']\n",
    "    actions = get_actions(start_date, end_date, all_actions)\n",
    "#     actions['date'] = pd.to_datetime(actions['time']).apply(lambda x: x.date())\n",
    "    df = pd.get_dummies(actions['type'], prefix='cate_action')\n",
    "    actions = pd.concat([actions[['cate','date']], df], axis=1)\n",
    "    # 按照类别-日期分组计算针对不同类别的各种行为某段时间的标准差\n",
    "    actions_date = actions.groupby(['cate','date']).sum()\n",
    "    actions_date = actions_date.unstack()\n",
    "    actions_date.fillna(0, inplace=True)\n",
    "    action_1 = np.std(actions_date['cate_action_1'], axis=1)\n",
    "    action_1 = action_1.to_frame()\n",
    "    action_1.columns = ['cate_action_1_std']\n",
    "    action_2 = np.std(actions_date['cate_action_2'], axis=1)\n",
    "    action_2 = action_2.to_frame()\n",
    "    action_2.columns = ['cate_action_2_std']\n",
    "    action_3 = np.std(actions_date['cate_action_3'], axis=1)\n",
    "    action_3 = action_3.to_frame()\n",
    "    action_3.columns = ['cate_action_3_std']\n",
    "    action_4 = np.std(actions_date['cate_action_4'], axis=1)\n",
    "    action_4 = action_4.to_frame()\n",
    "    action_4.columns = ['cate_action_4_std']\n",
    "    action_5 = np.std(actions_date['cate_action_5'], axis=1)\n",
    "    action_5 = action_5.to_frame()\n",
    "    action_5.columns = ['cate_action_5_std']\n",
    "    action_6 = np.std(actions_date['cate_action_6'], axis=1)\n",
    "    action_6 = action_6.to_frame()\n",
    "    action_6.columns = ['cate_action_6_std']\n",
    "    actions_date = pd.concat([action_1, action_2, action_3, action_4, action_5, action_6], axis=1)\n",
    "    actions_date['cate'] = actions_date.index\n",
    "    # 按照类别分组，统计各个商品类别下行为的转化率\n",
    "    actions = actions.groupby(['cate'], as_index=False).sum()\n",
    "    days_interal = (datetime.strptime(end_date, '%Y-%m-%d')-datetime.strptime(start_date, '%Y-%m-%d')).days\n",
    "    \n",
    "#     actions['cate_action_1_ratio'] = actions['cate_action_4'] / actions['cate_action_1']\n",
    "#     actions['cate_action_2_ratio'] = actions['cate_action_4'] / actions['cate_action_2']\n",
    "#     actions['cate_action_3_ratio'] = actions['cate_action_4'] / actions['cate_action_3']\n",
    "#     actions['cate_action_5_ratio'] = actions['cate_action_4'] / actions['cate_action_5']\n",
    "#     actions['cate_action_6_ratio'] = actions['cate_action_4'] / actions['cate_action_6']\n",
    "    actions['cate_action_1_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_1']))\n",
    "    actions['cate_action_2_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_2']))\n",
    "    actions['cate_action_3_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_3']))\n",
    "    actions['cate_action_5_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_5']))\n",
    "    actions['cate_action_6_ratio'] =(np.log(1 + actions['cate_action_4']) - np.log(1 + actions['cate_action_6']))\n",
    "    # 按照类别分组，统计各个商品类别下行为在一段时间的均值\n",
    "    actions['cate_action_1_mean'] = actions['cate_action_1'] /  days_interal\n",
    "    actions['cate_action_2_mean'] = actions['cate_action_2'] /  days_interal\n",
    "    actions['cate_action_3_mean'] = actions['cate_action_3'] /  days_interal\n",
    "    actions['cate_action_4_mean'] = actions['cate_action_4'] /  days_interal\n",
    "    actions['cate_action_5_mean'] = actions['cate_action_5'] /  days_interal\n",
    "    actions['cate_action_6_mean'] = actions['cate_action_6'] /  days_interal\n",
    "    actions = pd.merge(actions, actions_date, how ='left',on='cate')\n",
    "    actions = actions[feature]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练集/测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造训练集/验证集\n",
    "* 标签,采用滑动窗口的方式，构造训练集的时候针对产生购买的行为标记为1\n",
    "* 整合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.383390Z",
     "start_time": "2017-05-16T21:16:19.318110Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(start_date, end_date, all_actions):\n",
    "    actions = get_actions(start_date, end_date, all_actions)\n",
    "#     actions = actions[actions['type'] == 4]\n",
    "    # 修改为预测购买了商品8的用户预测\n",
    "    actions = actions[(actions['type'] == 4) & (actions['cate']==8)]\n",
    "    \n",
    "    actions = actions.groupby(['user_id', 'sku_id'], as_index=False).sum()\n",
    "    actions['label'] = 1\n",
    "    actions = actions[['user_id', 'sku_id', 'label']]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.439236Z",
     "start_time": "2017-05-16T21:16:19.384369Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_actions(user, product, all_actions, train_start_date):\n",
    "    train_end_date = datetime.strptime(train_start_date, '%Y-%m-%d') + timedelta(days=3)\n",
    "    train_end_date = train_end_date.strftime('%Y-%m-%d')\n",
    "    # 修正prod_acc,cate_acc的时间跨度\n",
    "    start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30)\n",
    "    start_days = start_days.strftime('%Y-%m-%d')\n",
    "    print train_end_date\n",
    "    user_acc = get_recent_user_feat(train_end_date, all_actions)\n",
    "    print 'get_recent_user_feat finsihed'\n",
    "    \n",
    "    user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions)\n",
    "    print 'get_user_cate_feature finished'\n",
    "    \n",
    "    product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions)\n",
    "    print 'get_accumulate_product_feat finsihed'\n",
    "    cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions)\n",
    "    print 'get_accumulate_cate_feat finsihed'\n",
    "    comment_acc = get_comments_product_feat(train_end_date)\n",
    "    print 'get_comments_product_feat finished'\n",
    "    # 标记\n",
    "    test_start_date = train_end_date\n",
    "    test_end_date = datetime.strptime(test_start_date, '%Y-%m-%d') + timedelta(days=5)\n",
    "    test_end_date = test_end_date.strftime('%Y-%m-%d')\n",
    "    labels = get_labels(test_start_date, test_end_date, all_actions)\n",
    "    print \"get labels\"\n",
    "    \n",
    "    actions = None\n",
    "    for i in (3, 5, 7, 10, 15, 21, 30):\n",
    "        start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i)\n",
    "        start_days = start_days.strftime('%Y-%m-%d')\n",
    "        if actions is None:\n",
    "            actions = get_action_feat(start_days, train_end_date, all_actions, i)\n",
    "        else:\n",
    "            # 注意这里的拼接key\n",
    "            actions = pd.merge(actions, get_action_feat(start_days, train_end_date, all_actions, i), how='left',\n",
    "                               on=['user_id', 'sku_id', 'cate'])\n",
    "\n",
    "    actions = pd.merge(actions, user, how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_acc, how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_cate, how='left', on='user_id')\n",
    "    # 注意这里的拼接key\n",
    "    actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate'])\n",
    "    actions = pd.merge(actions, product_acc, how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, cate_acc, how='left', on='cate')\n",
    "    actions = pd.merge(actions, comment_acc, how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, labels, how='left', on=['user_id', 'sku_id'])\n",
    "    # 主要是填充拼接商品基本特征、评论特征、标签之后的空值\n",
    "    actions = actions.fillna(0)\n",
    "#     return actions\n",
    "    # 采样\n",
    "    action_postive = actions[actions['label'] == 1]\n",
    "    action_negative = actions[actions['label'] == 0]\n",
    "    del actions\n",
    "    neg_len = len(action_postive) * 10\n",
    "    action_negative = action_negative.sample(n=neg_len)\n",
    "    action_sample = pd.concat([action_postive, action_negative], ignore_index=True)    \n",
    "    \n",
    "    return action_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T21:16:19.509168Z",
     "start_time": "2017-05-16T21:16:19.440133Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_set(train_start_date, setNums ,f_path):\n",
    "    train_actions = None\n",
    "    all_actions = get_all_action()\n",
    "    print \"get all actions!\"\n",
    "    user = get_basic_user_feat()\n",
    "    print 'get_basic_user_feat finsihed'\n",
    "    product = get_basic_product_feat()\n",
    "    print 'get_basic_product_feat finsihed'\n",
    "    # 滑窗,构造多组训练集/验证集\n",
    "    for i in range(setNums):\n",
    "        print train_start_date\n",
    "        if train_actions is None:\n",
    "            train_actions = make_actions(user, product, all_actions, train_start_date)\n",
    "        else:\n",
    "            train_actions = pd.concat([train_actions, make_actions(user, product, all_actions, train_start_date)],\n",
    "                                          ignore_index=True)\n",
    "        # 接下来每次移动一天\n",
    "        train_start_date = datetime.strptime(train_start_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "        train_start_date = train_start_date.strftime('%Y-%m-%d')\n",
    "        print \"round {0}/{1} over!\".format(i+1, setNums)\n",
    "\n",
    "    train_actions.to_csv(f_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T22:34:29.544825Z",
     "start_time": "2017-05-16T21:16:19.510386Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all actions!\n",
      "get_basic_user_feat finsihed\n",
      "get_basic_product_feat finsihed\n",
      "2016-03-01\n",
      "2016-03-04\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 1/34 over!\n",
      "2016-03-02\n",
      "2016-03-05\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 2/34 over!\n",
      "2016-03-03\n",
      "2016-03-06\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 3/34 over!\n",
      "2016-03-04\n",
      "2016-03-07\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 4/34 over!\n",
      "2016-03-05\n",
      "2016-03-08\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 5/34 over!\n",
      "2016-03-06\n",
      "2016-03-09\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 6/34 over!\n",
      "2016-03-07\n",
      "2016-03-10\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 7/34 over!\n",
      "2016-03-08\n",
      "2016-03-11\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 8/34 over!\n",
      "2016-03-09\n",
      "2016-03-12\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 9/34 over!\n",
      "2016-03-10\n",
      "2016-03-13\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 10/34 over!\n",
      "2016-03-11\n",
      "2016-03-14\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 11/34 over!\n",
      "2016-03-12\n",
      "2016-03-15\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 12/34 over!\n",
      "2016-03-13\n",
      "2016-03-16\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 13/34 over!\n",
      "2016-03-14\n",
      "2016-03-17\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 14/34 over!\n",
      "2016-03-15\n",
      "2016-03-18\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 15/34 over!\n",
      "2016-03-16\n",
      "2016-03-19\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 16/34 over!\n",
      "2016-03-17\n",
      "2016-03-20\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 17/34 over!\n",
      "2016-03-18\n",
      "2016-03-21\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 18/34 over!\n",
      "2016-03-19\n",
      "2016-03-22\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 19/34 over!\n",
      "2016-03-20\n",
      "2016-03-23\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 20/34 over!\n",
      "2016-03-21\n",
      "2016-03-24\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 21/34 over!\n",
      "2016-03-22\n",
      "2016-03-25\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 22/34 over!\n",
      "2016-03-23\n",
      "2016-03-26\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 23/34 over!\n",
      "2016-03-24\n",
      "2016-03-27\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 24/34 over!\n",
      "2016-03-25\n",
      "2016-03-28\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 25/34 over!\n",
      "2016-03-26\n",
      "2016-03-29\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 26/34 over!\n",
      "2016-03-27\n",
      "2016-03-30\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 27/34 over!\n",
      "2016-03-28\n",
      "2016-03-31\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 28/34 over!\n",
      "2016-03-29\n",
      "2016-04-01\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 29/34 over!\n",
      "2016-03-30\n",
      "2016-04-02\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 30/34 over!\n",
      "2016-03-31\n",
      "2016-04-03\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 31/34 over!\n",
      "2016-04-01\n",
      "2016-04-04\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 32/34 over!\n",
      "2016-04-02\n",
      "2016-04-05\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 33/34 over!\n",
      "2016-04-03\n",
      "2016-04-06\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get labels\n",
      "round 34/34 over!\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_start_date = '2016-03-01'\n",
    "make_train_set(train_start_date, 34, 'train_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造验证集(线下测试集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T22:34:29.585821Z",
     "start_time": "2017-05-16T22:34:29.546019Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_val_answer(val_start_date, val_end_date, all_actions, label_val_s1_path):\n",
    "    actions = get_actions(val_start_date, val_end_date,all_actions)\n",
    "    actions = actions[(actions['type'] == 4) & (actions['cate'] == 8)]\n",
    "    actions = actions[['user_id', 'sku_id']]\n",
    "    actions = actions.drop_duplicates()\n",
    "    actions.to_csv(label_val_s1_path, index=False)\n",
    "\n",
    "def make_val_set(train_start_date, train_end_date, val_s1_path):\n",
    "    # 修改时间跨度\n",
    "    start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30)\n",
    "    start_days = start_days.strftime('%Y-%m-%d')\n",
    "    all_actions = get_all_action()\n",
    "    print \"get all actions!\"\n",
    "    user = get_basic_user_feat()\n",
    "    print 'get_basic_user_feat finsihed'\n",
    "    \n",
    "    product = get_basic_product_feat()\n",
    "    print 'get_basic_product_feat finsihed'\n",
    "#     user_acc = get_accumulate_user_feat(train_end_date,all_actions,30)\n",
    "#     print 'get_accumulate_user_feat finished'\n",
    "    user_acc = get_recent_user_feat(train_end_date, all_actions)\n",
    "    print 'get_recent_user_feat finsihed'\n",
    "    user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions)\n",
    "    print 'get_user_cate_feature finished'\n",
    " \n",
    "    product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions)\n",
    "    print 'get_accumulate_product_feat finsihed'\n",
    "    cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions)\n",
    "    print 'get_accumulate_cate_feat finsihed'\n",
    "    comment_acc = get_comments_product_feat(train_end_date)\n",
    "    print 'get_comments_product_feat finished'\n",
    "    \n",
    "    actions = None\n",
    "    for i in (3, 5, 7, 10, 15, 21, 30):\n",
    "        start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i)\n",
    "        start_days = start_days.strftime('%Y-%m-%d')\n",
    "        if actions is None:\n",
    "            actions = get_action_feat(start_days, train_end_date, all_actions,i)\n",
    "        else:\n",
    "            actions = pd.merge(actions, get_action_feat(start_days, train_end_date,all_actions,i), how='left',\n",
    "                               on=['user_id', 'sku_id', 'cate'])\n",
    "\n",
    "    actions = pd.merge(actions, user, how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_acc, how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_cate, how='left', on='user_id')\n",
    "    # 注意这里的拼接key\n",
    "    actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate'])\n",
    "    actions = pd.merge(actions, product_acc, how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, cate_acc, how='left', on='cate')\n",
    "    actions = pd.merge(actions, comment_acc, how='left', on='sku_id')\n",
    "    actions = actions.fillna(0)\n",
    "   \n",
    "    \n",
    "#     print actions\n",
    "    # 构造真实用户购买情况作为后续验证\n",
    "    val_start_date = train_end_date\n",
    "    val_end_date = datetime.strptime(val_start_date, '%Y-%m-%d') + timedelta(days=5)\n",
    "    val_end_date = val_end_date.strftime('%Y-%m-%d')\n",
    "    make_val_answer(val_start_date, val_end_date, all_actions, 'label_'+val_s1_path)\n",
    "    \n",
    "    actions.to_csv(val_s1_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T22:44:56.918322Z",
     "start_time": "2017-05-16T22:34:29.587106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all actions!\n",
      "get_basic_user_feat finsihed\n",
      "get_basic_product_feat finsihed\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get all actions!\n",
      "get_basic_user_feat finsihed\n",
      "get_basic_product_feat finsihed\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n",
      "get all actions!\n",
      "get_basic_user_feat finsihed\n",
      "get_basic_product_feat finsihed\n",
      "get_recent_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n",
      "get_comments_product_feat finished\n"
     ]
    }
   ],
   "source": [
    "# 验证集\n",
    "# train_start_date = '2016-04-06'\n",
    "# make_train_set(train_start_date, 3, 'val_set.csv')\n",
    "make_val_set('2016-04-06', '2016-04-09', 'val_1.csv')\n",
    "make_val_set('2016-04-07', '2016-04-10', 'val_2.csv')\n",
    "make_val_set('2016-04-08', '2016-04-11', 'val_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T22:44:57.248609Z",
     "start_time": "2017-05-16T22:44:57.023515Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_set(train_start_date, train_end_date):\n",
    "    start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=30)\n",
    "    start_days = start_days.strftime('%Y-%m-%d')\n",
    "    all_actions = get_all_action()\n",
    "    print \"get all actions!\"\n",
    "    user = get_basic_user_feat()\n",
    "    print 'get_basic_user_feat finsihed'\n",
    "    product = get_basic_product_feat()\n",
    "    print 'get_basic_product_feat finsihed'\n",
    "    \n",
    "    user_acc = get_recent_user_feat(train_end_date, all_actions)\n",
    "    print 'get_accumulate_user_feat finsihed'\n",
    "    \n",
    "    user_cate = get_user_cate_feature(train_start_date, train_end_date, all_actions)\n",
    "    print 'get_user_cate_feature finished'\n",
    "    \n",
    "    product_acc = get_accumulate_product_feat(start_days, train_end_date, all_actions)\n",
    "    print 'get_accumulate_product_feat finsihed'\n",
    "    cate_acc = get_accumulate_cate_feat(start_days, train_end_date, all_actions)\n",
    "    print 'get_accumulate_cate_feat finsihed'\n",
    "    comment_acc = get_comments_product_feat(train_end_date)\n",
    "\n",
    "    actions = None\n",
    "    for i in (3, 5, 7, 10, 15, 21, 30):\n",
    "        start_days = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i)\n",
    "        start_days = start_days.strftime('%Y-%m-%d')\n",
    "        if actions is None:\n",
    "            actions = get_action_feat(start_days, train_end_date, all_actions,i)\n",
    "        else:\n",
    "            actions = pd.merge(actions, get_action_feat(start_days, train_end_date,all_actions,i), how='left',\n",
    "                               on=['user_id', 'sku_id', 'cate'])\n",
    "\n",
    "    actions = pd.merge(actions, user, how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_acc, how='left', on='user_id')\n",
    "    actions = pd.merge(actions, user_cate, how='left', on='user_id')\n",
    "    # 注意这里的拼接key\n",
    "    actions = pd.merge(actions, product, how='left', on=['sku_id', 'cate'])\n",
    "    actions = pd.merge(actions, product_acc, how='left', on='sku_id')\n",
    "    actions = pd.merge(actions, cate_acc, how='left', on='cate')\n",
    "    actions = pd.merge(actions, comment_acc, how='left', on='sku_id')\n",
    "\n",
    "    actions = actions.fillna(0)\n",
    "    \n",
    "\n",
    "    actions.to_csv(\"test_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.13~4.16这三天的评论记录似乎并不存在为0的情况，导致构建测试集时出错\n",
    "\n",
    "`KeyError: \"['comment_num_0'] not in index\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-16T22:48:22.214337Z",
     "start_time": "2017-05-16T22:44:57.250749Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all actions!\n",
      "get_basic_user_feat finsihed\n",
      "get_basic_product_feat finsihed\n",
      "get_accumulate_user_feat finsihed\n",
      "get_user_cate_feature finished\n",
      "get_accumulate_product_feat finsihed\n",
      "get_accumulate_cate_feat finsihed\n"
     ]
    }
   ],
   "source": [
    "# 预测结果\n",
    "sub_start_date = '2016-04-13'\n",
    "sub_end_date = '2016-04-16'\n",
    "make_test_set(sub_start_date, sub_end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
